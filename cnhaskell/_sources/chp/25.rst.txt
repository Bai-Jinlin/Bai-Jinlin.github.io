第 25 章：性能剖析与优化
==============================

..
    Haskell is a high level language. A really high level language.
    We can spend our days programming entirely in abstractions, in monoids,
    functors and hylomorphisms, far removed from any particular hardware model
    of computation.
    The language specification goes to great lengths to avoid prescribing
    any particular evaluation model.
    These layers of abstraction let us treat Haskell as a notation for computation itself,
    letting the programmer concentrate on the essence of their problem without
    getting bogged down in low level implementation decisions.
    We get to program in pure thought.

Haskell 是一门高级编程语言，一门真正的高级编程语言。 我们可以一直使用抽象概念、
幺半群、函子、以及多态进行编程，而不必与任何特定的硬件模型打交道。
Haskell 在语言规范方面下了很大的功夫，力求语言可以不受制于某个特定的求值模型。
这几层抽象使得我们可以把 Haskell 作为计算本身的记号，
让编程人员关心他们问题的关键点，而不用操心低层次的实现细节，
使得人们可以心无旁骛地进行编程。

..
    However, this is a book about real world programming, and in the real world,
    code runs on stock hardware with limited resources.
    Our programs will have time and space requirements that we may need to enforce.
    As such, we need a good knowledge of how our program data is represented,
    the precise consequences of using lazy or strict evaluation strategies,
    and techniques for analyzing and controlling space and time behavior.

但是，本书介绍的是真实世界中的编程行为，而真实世界中的代码都运行在资源有限的硬件之上，
并且程序也会有时间和空间上的限制。
因此，我们需要掌握好程序数据的底层结构，准确地理解使用惰性求值和严格求值策略带来的后果，
并学会如何分析和控制程序在时间和空间上的行为。

..
    In this chapter we'll look at typical space and time problems a Haskell programmer
    might encounter, and how to methodically analyse, understand and address them.
    To do this we'll use investigate a range of techniques: time and space profiling,
    runtime statistics, and reasoning about strict and lazy evaluation.
    We'll also look at the impact of compiler optimizations on performance,
    and the use of advanced optimization techniques that become feasible
    in a purely functional language. So let's begin with a challenge:
    squashing unexpected memory usage in some inoccuous looking code.

在这一章，我们将会去看一些 Haskell 编程中常见的空间和时间问题，并且学习如何对它们进行有条理地分析，
最后理解并解决它们。
为此我们将研究使用一系列的技术：时间和空间性能剖析，运行时统计，以及对严格求值和惰性求值进行推断。
我们也会看下编译器优化对性能的影响，以及在纯函数式编程语言中可行的高级优化技术的应用。
那么，让我们用一个挑战开始吧：调查一个看似无害的程序中出乎意料的内存使用的问题。


Haskell 程序性能剖析
-------------------

..
    Let's consider the following list manipulating program,
    which naively computes the mean of some large list of values.
    While only a program fragment (and we'll stress that the particular algorithm
    we're implementing is irrelevant here),
    it is representative of real code we might find in any Haskell program:
    typically concise list manipulation code,
    and heavy use of standard library functions.
    It also illustrates several common performance trouble spots that can catch out the unwary.

请看下面这个列表处理程序，它用于计算某个超长列表的平均值。
这里展示的只是程序的其中一部分代码（并且具体的实现算法我们并不关心），
这是我们经常会在真实的 Haskell 程序中看到的典型的简单列表操作代码，
这些代码大量地使用标准库函数，并且包含了一些因为疏忽大意而导致的性能问题。
这里也展示了几种因疏忽而易出现的性能问题。

::

    -- file: ch25/A.hs
    import System.Environment
    import Text.Printf

    main = do
        [d] <- map read `fmap` getArgs
        printf "%f\n" (mean [1..d])

    mean :: [Double] -> Double
    mean xs = sum xs / fromIntegral (length xs)


..
    This program is very simple: we import functions for accessing
    the system's environment (in particular, getArgs),
    and the Haskell version of printf, for formatted text output.
    The program then reads a numeric literal from the command line,
    using that to build a list of floating point values,
    whose mean value we compute by dividing the list sum by its length.
    The result is printed as a string.
    Let's compile this source to native code (with optimizations on)
    and run it with the time command to see how it performs:

这个程序非常简单：我们引用了访问系统环境的函数（即 ``getArgs`` ），
和 Haskell 版的 ``printf`` 函数来格式化输出。接着这个程序从命令行
读入一个数字来构建一个由浮点数组成的列表。我们用这个列表的和除以列表的
长度得到平均值，然后以字符串的形式打印出来。我们来将此代码编译成机器代码（打开优化开关）
然后用 ``time`` 命令执行它看看情况吧：

::

    $ ghc --make -rtsopts -O2 A.hs
    [1 of 1] Compiling Main             ( A.hs, A.o )
    Linking A ...
    $ time ./A 1e5
    50000.5
    ./A 1e5  0.05s user 0.01s system 102% cpu 0.059 total
    $ time ./A 1e6
    500000.5
    ./A 1e6  0.26s user 0.04s system 99% cpu 0.298 total
    $ time ./A 1e7
    5000000.5
    ./A 1e7  63.80s user 0.62s system 99% cpu 1:04.53 total

..
    It worked well for small numbers,
    but the program really started to struggle with input size of ten million.
    From this alone we know something's not quite right,
    but it's unclear what resources are being used. Let's investigate.

程序在处理少量元素时运行得非常好，但是当输入的列表元素数量达到一千万个时，
程序的运行速度就会变得相当缓慢。从这点我们就能感觉到应该有什么地方做得不对，
但我们并不清楚它的资源使用情况。 我们需要研究下。

..
    Collecting runtime statistics
    +++++++++++++++++++++++++++++

收集运行时统计信息
++++++++++++++

..
    To get access to that kind of information,
    GHC lets us pass flags directly to the Haskell runtime, using the special
    +RTS and -RTS flags to delimit arguments reserved for the runtime system.
    The application itself won't see those flags, as they're immediately
    consumed by the Haskell runtime system.

为了获得这些信息，GHC 支持直接向 Haskell 运行时传入一些标志(flags)，如 +RTS 和 -RTS。
这些特殊的标志是传递给 Haskell 的运行时系统的保留参数，会直接被运行时系统处理掉。
而应用程序本身并不会看到这些标志。

..
    In particular, we can ask the runtime system to gather memory and
    garbage collector performance numbers with the -s flag (as well as
    control the number of OS threads with -N, or tweak the stack and heap sizes).
    We'll also use runtime flags to enable different varieties of profiling.
    The complete set of flags the Haskell runtime accepts is documented in the
    GHC User's Guide:

特别地，我们可以用 ``-s`` 标志来让运行时系统收集内存和垃圾收集器(garbage collector，
以下也会用 GC 来简称)的性能参数
（并可以用 ``-N`` 来控制系统线程的数量，或调整栈和堆的大小）。
我们将用各种运行时标志来启动不同的性能剖析。
Haskell 运行时能够接受的所有标志列表可以参见 GHC 用户手册。

..
    So let's run the program with statistic reporting enabled,
    via +RTS -sstderr, yielding this result.

那么让我们用 ``+RTS -sstderr`` 来运行程序取得我们所需要的结果吧。

::

    $ ./A 1e7 +RTS -sstderr
    5000000.5
    1,689,133,824 bytes allocated in the heap
    697,882,192 bytes copied during GC (scavenged)
    465,051,008 bytes copied during GC (not scavenged)
    382,705,664 bytes maximum residency (10 sample(s))

           3222 collections in generation 0 (  0.91s)
             10 collections in generation 1 ( 18.69s)

            742 Mb total memory in use

      INIT  time    0.00s  (  0.00s elapsed)
      MUT   time    0.63s  (  0.71s elapsed)
      GC    time   19.60s  ( 20.73s elapsed)
      EXIT  time    0.00s  (  0.00s elapsed)
      Total time   20.23s  ( 21.44s elapsed)

      %GC time      96.9%  (96.7% elapsed)

      Alloc rate    2,681,318,018 bytes per MUT second

      Productivity   3.1% of total user, 2.9% of total elapsed

..
    When using -sstderr, our program's performance numbers are printed to the standard
    error stream, giving us a lot of information about what our program was doing.
    In particular, it tells us how much time was spent in garbage collection,
    and what the maximum live memory usage was.
    It turns out that to compute the mean of a list of 10 million elements
    our program used a maximum of 742 megabytes on the heap,
    and spent 96.9% of its time doing garbage collection! In total,
    only 3.1% of the program's running time was spent doing productive work.

当使用 ``-sstderr`` 时，程序的性能数字会输出到标准错误流里，告诉我们很多关于程序
在具体做什么的信息。尤其是，它告诉了我们 GC 占用了多少时间，以及最大活动内存的使用情况。
原来，为了计算一千万个元素的平均值，程序在堆上使用了多达 742M 的内存，并且 96.9% 的时间是
花费到垃圾收集器上的！总的来说，只有 3.1% 的时间是程序用来干正事的。

..
    So why is our program behaving so badly, and what can we do to improve it?
    After all, Haskell is a lazy language: shouldn't it be able to process the list
    in constant space?

那么为什么我们的程序运行情况这么差？我们如何来提高它呢？毕竟，Haskell 是一个惰性语言：
它不应该只用恒定的内存空间来处理列表吗？

..
    Time profiling
    ++++++++++++++

时间剖析
++++++++

..
    GHC, thankfully, comes with several tools to analyze a program's time and space usage.
    In particular, we can compile a program with profiling enabled, which, when run,
    yields useful information about what resources each function was using.
    Profiling proceeds in three steps: compiling the program for profiling;
    running it with particular profiling modes enabled; and inspecting the resulting statistics.

幸运的是，GHC 为我们提供了多种工具来剖析程序的时间和空间使用情况。
我们可以在编译程序时打开性能剖析标志，这样程序在执行时会生成每个函数的资源使用信息。
性能剖析可以按照下面三个步骤来进行：
用性能剖析标志编译程序；执行程序时用运行时标志打开特定的性能剖析模式；最后分析收集到的统计信息。

..
    To compile our program for basic time and allocation profiling, we use the -prof flag.
    We also need to tell the profiling code which functions we're interested in profiling,
    by adding "cost centres" to them.
    A cost centre is a location in the program we'd like to collect statistics about,
    and GHC will generate code to compute the cost of evaluating the expression at each location.
    Cost centres can be added manually to instrument any expression, using the SCC pragma:

编译程序时，我们可以使用 ``-prof`` 性能剖析标志来得到基本的时间和空间消耗信息。
我们也需要给感兴趣的函数标记为“消耗集中点(cost centres)”以便让性能剖析程序知晓。
一个消耗集中点即是一个信息收集点，GHC 会生成代码来计算在这些地方执行的表达式的消耗情况。
我们可以用 ``SCC`` 编译指示(pragma)把任何表达式设为消耗集中点。

::

    -- file: ch25/SCC.hs
    mean :: [Double] -> Double
    mean xs = {-# SCC "mean" #-} sum xs / fromIntegral (length xs)

..
    Alternatively, we can have the compiler insert the cost centres on
    all top level functions for us by compiling with the -auto-all flag.
    Manual cost centres are a useful addition to automated cost centre profiling,
    as once a hot spot has been identified, we can precisely pin down the
    expensive sub-expressions of a function.

或者，我们也可以用 ``-auto-all`` 标志来让编译器将所有顶层函数设为消耗集中点。
然后在我们识别出某个性能热点(hot spot)的函数之后，把手动添加消耗集中点作为一个十分有用的补充，
就可以更为精确地去侦测该函数的子表达式了。

..
    One complication to be aware of: in a lazy, pure language like Haskell,
    values with no arguments need only be computed once
    (for example, the large list in our example program),
    and the result shared for later uses.
    Such values are not really part of the call graph of a program,
    as they're not evaluated on each call, but we would of course still like to
    know how expensive their one-off cost of evaluation was.
    To get accurate numbers for these values, known as "constant applicative forms",
    or CAFs, we use the -caf-all flag.

需要注意的一个复杂的地方：在 Haskell 这类惰性、纯函数式编程语言里，
没有参数的值只会被计算一次（比如之前计算超长列表的程序），然后计算的结果会在之后共享。
于是这种函数在调用关系图(call graph)中记录的统计值并不准确，因为它们并不是每次调用都执行。
然而，我们仍然想要知道它们一次执行的消耗情况是怎么样。
为了得到这种被称为“常量函数体(Constant Applicative Forms)”或 CAF 的确切值，
我们可以使用 ``-caf-all`` 标志。

..
    Compiling our example program for profiling then (using the -fforce-recomp flag
    to to force full recompilation):

那么以性能剖析的方式来编译我们的程序吧（用 ``-fforce-recomp`` 标志来强制重新编译所有部分）：

::

    $ ghc -O2 --make A.hs -prof -auto-all -caf-all -fforce-recomp
    [1 of 1] Compiling Main             ( A.hs, A.o )
    Linking A ...


..
    We can now run this annotated program with time profiling enabled
    (and we'll use a smaller input size for the time being,
    as the program now has additional profiling overhead):

现在我们可以执行这个标记了性能剖析点的程序了 （标记了性能剖析的程序会
变慢，所以我们用一个较小的输入来执行）：

::

    $ time ./A  1e6 +RTS -p
    Stack space overflow: current size 8388608 bytes.
    Use `+RTS -Ksize' to increase it.
    ./A 1e6 +RTS -p  1.11s user 0.15s system 95% cpu 1.319 total

..
    The program ran out of stack space! This is the main complication to
    be aware of when using profiling: adding cost centres to a program modifies
    how it is optimized, possibly changing its runtime behavior, as each
    expression now has additional code associated with it to track the
    evaluation steps. In a sense, observing the program executing modifies
    how it executes. In this case, it is simple to proceed
    -- we use the GHC runtime flag, -K, to set a larger stack limit for
    our program (with the usual suffixes to indicate magnitude):

程序竟然把栈空间耗完了！这就是使用性能剖析时需要注意的主要影响：
给程序加消耗集中点会改变它的优化方式，进而可能影响它的运行时表现，
因为每一个被标记的表达式都会被附加一段额外的代码，以此来跟踪它们的执行轨迹。
从某种意义上说，观察程序执行会影响它的执行。
对于我们这样情况，修正起来很简单——只需要通过 GHC 运行时标志 ``-K`` 来增加
栈空间上限即可（要附带指示存储单位的后缀）：

::

    $ time ./A 1e6 +RTS -p -K100M
    500000.5
    ./A 1e6 +RTS -p -K100M  4.27s user 0.20s system 99% cpu 4.489 total


..
    The runtime will dump its profiling information into a file,
    A.prof (named after the binary that was executed)
    which contains the following information:

运行时会将性能信息生成到一个名字为 ``A.prof``（以可执行程序的名字命名）的文件中。
其中含有以下信息：

::

    $ cat A.prof

    Time and Allocation Profiling Report  (Final)

           A +RTS -p -K100M -RTS 1e6

        total time  =        0.28 secs   (14 ticks @ 20 ms)
        total alloc = 224,041,656 bytes  (excludes profiling overheads)

    COST CENTRE  MODULE               %time %alloc

    CAF:sum      Main                  78.6   25.0
    CAF          GHC.Float             21.4   75.0

                                                individual    inherited
    COST CENTRE MODULE         no.    entries  %time %alloc   %time %alloc

    MAIN        MAIN            1           0   0.0    0.0   100.0  100.0
     main       Main          166           2   0.0    0.0     0.0    0.0
      mean      Main          168           1   0.0    0.0     0.0    0.0
     CAF:sum    Main          160           1  78.6   25.0    78.6   25.0
     CAF:lvl    Main          158           1   0.0    0.0     0.0    0.0
      main      Main          167           0   0.0    0.0     0.0    0.0
     CAF        Numeric       136           1   0.0    0.0     0.0    0.0
     CAF        Text.Read.Lex 135           9   0.0    0.0     0.0    0.0
     CAF        GHC.Read      130           1   0.0    0.0     0.0    0.0
     CAF        GHC.Float     129           1  21.4   75.0    21.4   75.0
     CAF        GHC.Handle    110           4   0.0    0.0     0.0    0.0


..
    This gives us a view into the program's runtime behavior.
    We can see the program's name and the flags we ran it with.
    The "total time" is time actually spent executing code from the
    runtime system's point of view, and the total allocation is the number
    of bytes allocated during the entire program run
    (not the maximum live memory, which was around 700MB).

这些信息给我们描述了程序在运行时的行为。里面包含了程序的名字以及
执行程序时用到的标志和参数。“total time”是运行时系统视角所见的程序运行
的确切总时长。“total alloc”是程序在运行过程中分配的内存总字节数（不是
程序运行时内存使用的峰值；那个峰值大概是 700MB）

..
    The second section of the profiling report is the proportion of time and
    space each function was responsible for. The third section is the cost centre report,
    structured as a call graph (for example, we can see that mean was called from main.
    The "individual" and "inherited" columns give us the resources a cost centre
    was responsible for on its own, and what it and its children were responsible for.
    Additionally, we see the one-off costs of evaluating constants
    (such as the floating point values in the large list, and the list itself)
    assigned to top level CAFs.

报告中的第二段是各个函数所消耗的时间和空间部分。
第三段是消耗集中点的报告，它被组织成了调用关系图的格式
（比如，我们可以看出 ``mean`` 是被 ``main`` 调用的）。
“individual”和“inherited”列提供了每个消耗集中点其本身、以及它和它的子部分
所消耗的资源。此外，最下面那些 ``CAF`` 是常量执行的一次性消耗（例如超长列表中
浮点数以及列表本身）。

..
    What conclusions can we draw from this information? We can see that the
    majority of time is spent in two CAFs, one related to computing the sum,
    and another for floating point numbers. These alone account for nearly
    all allocations that occurred during the program run. Combined with our
    earlier observation about garbage collector stress, it begins to look like
    the list node allocations, containing floating point values,
    are causing a problem.

我们能从这些信息得出什么结论呢？我们可以看出两个 ``CAF`` 占用了大多数时间：
一个与计算总和相关，另一个与浮点数相关。
光是它们就几乎占据了程序运行期间的所有消耗。
结合我们之前观察到 GC 的压力问题，看起来像是在列表节点的内存分配和浮点数值上发生了问题。

..
    For simple performance hot spot identification, particularly in large
    programs where we might have little idea where time is being spent,
    the initial time profile can highlight a particular problematic module
    and top level function, which is often enough to reveal the trouble spot.
    Once we've narrowed down the code to a problematic section,
    such as our example here, we can use more sophisticated profiling
    tools to extract more information.

简单的性能热点检测，特别是对于我们难以知道时间花费点的大型程序，
这个时间剖析会突出某些问题模块或顶层函数。这往往已足够显示出问题所在了。
跟上面展示的程序一样，一旦我们缩小了问题代码的范围，我们就可以用更加尖端的剖析工具来拿到更多的信息。

..
    Space profiling
    +++++++++++++++

空间剖析
++++++++

..
    Beyond basic time and allocation statistics, GHC is able to generate
    graphs of memory usage of the heap, over the program's lifetime.
    This is perfect for revealing "space leaks", where memory is retained
    unnecessarily, leading to the kind of heavy garbage collector activity
    we see in our example.

GHC 除了可以进行基本的时间和空间统计外，还能为程序整个执行期间的堆内存使用
情况生成图表。这能完美检测内存泄露问题。内存泄露是指不再需要的内存没有被释放。
这会对 GC 造成压力，就像在我们的程序中见到的那样。

..
    Constructing a heap profile follows the same steps as constructing
    a normal time profile, namely, compile with -prof -auto-all -caf-all,
    but when we execute the program, we'll ask the runtime system to gather
    more detailed heap use statistics. We can break down the heap use
    information in several ways: via cost-centre, via module, by constructor,
    by data type, each with its own insights. Heap profiling A.hs logs
    to a file A.hp, with raw data which is in turn processed by the tool
    hp2ps, which generates a PostScript-based, graphical visualization
    of the heap over time.

构建堆内存剖析和构建一般的时间剖析的步骤是一样的，
都需要用到 ``-prof -auto-all -caf-all`` 编译标志。 但当执行程序时，
我们会让运行时系统收集关于堆使用的最多细节。堆使用信息能够以几种方式分解：
消耗集中点、模块、构造器和数据类型。每个都有各自的洞见。
对 ``A.hs`` 进行堆内存剖析所得的原始数据会被记录到一个名为 ``A.hp`` 的文件里面，
之后只要使用 ``hp2ps`` 处理这个文件，
就可以得到一个堆内存占用历史图像的 PostScript 文件。

..
    To extract a standard heap profile from our program, we run it with
    the -hc runtime flag:

想要使用标准的堆内存剖析的话，可以在运行程序时使用 ``-hc`` 作为运行时的性能剖析标志：

::

    $ time ./A 1e6 +RTS -hc -p -K100M
    500000.5
    ./A 1e6 +RTS -hc -p -K100M  4.15s user 0.27s system 99% cpu 4.432 total


..
    A heap profiling log, A.hp, was created, with the content
    in the following form:

一个堆内存剖析的日志文件 ``A.hp`` 会创建出来，其内容为以下形式：

::

    JOB "A 1e6 +RTS -hc -p -K100M"
    SAMPLE_UNIT "seconds"
    VALUE_UNIT "bytes"
    BEGIN_SAMPLE 0.00
    END_SAMPLE 0.00
    BEGIN_SAMPLE 0.24
    (167)main/CAF:lvl   48
    (136)Numeric.CAF    112
    (166)main   8384
    (110)GHC.Handle.CAF 8480
    (160)CAF:sum    10562000
    (129)GHC.Float.CAF  10562080
    END_SAMPLE 0.24

..
    Samples are taken at regular intervals during the program run.
    We can increase the heap sampling frequency by using -iN, where N is
    the number of seconds (e.g. 0.01) between heap size samples. Obviously,
    the more we sample, the more accurate the results, but the slower
    our program will run. We can now render the heap profile as a graph,
    using the hp2ps tool:

这些样本是在程序运行期间以固定的间隔采样出来的。
我们可以用 ``-iN`` 标志来增加采样频率，
这里的 ``N`` 是内存采样之间相隔的秒数（如 0.01 秒）。
很明显，采样越频繁，得到的结果越精确，但程序也会执行得越慢。
我们可以用 ``hp2ps`` 将剖析结果生成一张图表：

::

    $ hp2ps -e8in -c A.hp

..
    This produces the graph, in the file A.ps:

这就是生成的图表 ``A.ps`` ：

..
    [img here]

.. image:: ../image/ch25/ch25-heap-hc.png

..
    What does this graph tell us? For one, the program runs in two phases:
    spending its first half allocating increasingly large amounts of memory,
    while summing values, and the second half cleaning up those values.
    The initial allocation also coincides with sum, doing some work,
    allocating a lot of data. We get a slightly different presentation if
    we break down the allocation by type, using -hy profiling:

我们能从图片中看出什么？举个例子，程序的执行分为两个阶段：
前一阶段在计算数值总和的同时不断分配大量的内存，后一阶段清理释放这些内存。
内存初始化分配的同时， ``sum`` 也开始工作，并消耗大量的内存。
如果用性能剖析标志 ``-hy`` 来按类型分解的话，我们会得到一个稍有不同的图像：

::

    $ time ./A 1e6 +RTS -hy -p -K100M
    500000.5
    ./A 1e6 +RTS -i0.001 -hy -p -K100M  34.96s user 0.22s system 99% cpu 35.237 total
    $ hp2ps -e8in -c A.hp

..
    Which yields the following graph:

以下是生成的图像：

..
    [img here]

.. image:: ../image/ch25/ch25-heap-hy.png

..
    The most interesting things to notice here are large parts of the heap
    devoted to values of list type (the [] band), and heap-allocated
    Double values. There's also some heap allocated data of unknown type
    (represented as data of type "*"). Finally, let's break it down by
    what constructors are being allocated, using the -hd flag:

这里最有趣的是很大部分的内存都被 list 类型（即“[]”）和 Double 类型所占用；
我们看到未知类型（图中用“*”标记）也占用了一些内存。
最后，再让我们用 ``-hd`` 标志，根据构造器对结果进行分解：

::

    $ time ./A 1e6 +RTS -hd -p -K100M
    $ time ./A 1e6 +RTS -i0.001 -hd -p -K100M
    500000.5
    ./A 1e6 +RTS -i0.001 -hd -p -K100M  27.85s user 0.31s system 99% cpu 28.222 total

..
    Our final graphic reveals the full story of what is going on:

下面就是能够展示程序执行的所有情况的最终图像：

..
    [img here]

.. image:: ../image/ch25/ch25-heap-hd.png

..
    A lot of work is going into allocating list nodes containing
    double-precision floating point values. Haskell lists are lazy,
    so the full million element list is built up over time. Crucially,
    though, it is not being deallocated as it is traversed, leading to
    increasingly large resident memory use. Finally, a bit over halfway
    through the program run, the program finally finishes summing the list,
    and starts calculating the length. If we look at the original
    fragment for mean, we can see exactly why that memory is being retained:

程序在分配双精度浮点数列表上面花了不少功夫。列表在 Haskell 语言中是惰性的，
所以含有上百万个元素的列表都是在程序执行过程中一点点地构建出来的。
但这些元素在被遍历的同时并没有被逐步释放，所以导致内存占用变得越来越大。
最终，在程序执行稍稍超过一半时，终于将列表总和计算出来，并开始计算其长度。
如果看下关于 ``mean`` 的程序片断的话，我们就会知道内存没被释放的确切原因：

::

    -- file: ch25/Fragment.hs
    mean :: [Double] -> Double
    mean xs = sum xs / fromIntegral (length xs)

..
    At first we sum our list, which triggers the allocation of list nodes,
    but we're unable to release the list nodes once we're done,
    as the entire list is still needed by length. As soon as sum is
    done though, and length starts consuming the list, the garbage
    collector can chase it along, deallocating the list nodes,
    until we're done. These two phases of evaluation give two strikingly
    different phases of allocation and deallocation, and point at exactly
    what we need to do: traverse the list only once, summing and averaging
    it as we go.

首先我们计算列表的总和，这会使得所有列表元素被分配到内存。
但我们现在还不能释放列表元素，因为 ``length`` 还需要整个列表。
一旦 ``sum`` 结束， ``length`` 会马上开始访问列表，同时 GC 会跟进，
逐步释放列表元素，直到 ``length`` 结束。
这两个计算阶段展示了两种明显不同的分配与释放，并指出我们需要改进的确切思路：
只对列表遍历一次，遍历过程中同时计算总和与平均值。

..
    Controlling evaluation
    ----------------------

控制求值
---------

..
    We have a number of options if we want to write our loop to traverse
    the list only once. For example, we can write the loop as a fold over
    the list, or via explicit recursion on the list structure. Sticking
    to the high level approaches, we'll try a fold first:

如果我们有很多方式来实现一个只遍历一次的循环。
例如，我们可以写一个对列表折叠(fold)的循环，也可以对列表进行显式的递归。
本着使用更高级的方法去解决问题的原则，我们决定先尝试折叠的方式：

::

    -- file: ch25/B.hs
    mean :: [Double] -> Double
    mean xs = s / fromIntegral n
      where
        (n, s)     = foldl k (0, 0) xs
        k (n, s) x = (n+1, s+x)

..
    Now, instead of taking the sum of the list, and retaining the list
    until we can take its length, we left-fold over the list,
    accumulating the intermediate sum and length values in a pair
    (and we must left-fold, since a right-fold would take us to the
    end of the list and work backwards, which is exactly what we're
    trying to avoid).

这次，我们不再采取“计算列表的总和，并保留这个列表直到获取它的长度为止”这一方案，
而是左折叠(left-fold)整个列表，累加当前的总和及长度到 pair(对组)上
（我们必须采用左折叠，因为右折叠(right-fold)会先带我们到列表的结尾，然后倒回来计算，
这恰恰是我们想要避免的）。

..
    The body of our loop is the k function, which takes the intermediate
    loop state, and the current element, and returns a new state with
    the length increased by one, and the sum increased by the current
    element. When we run this, however, we get a stack overflow:

循环的主体是 ``k`` 函数，把即时的循环状态和当前的列表元素作为参数，然后返回新状态——长度增一、总和加当前元素。然而，当我们运行它时发生了栈溢出：

::

    $ ghc -O2 --make B.hs -fforce-recomp
    $ time ./B 1e6
    Stack space overflow: current size 8388608 bytes.
    Use `+RTS -Ksize' to increase it.
    ./B 1e6  0.44s user 0.10s system 96% cpu 0.565 total


..
    We traded wasted heap for wasted stack! In fact, if we increase
    the stack size to the size of the heap in our previous implementation,
    with the -K runtime flag, the program runs to completion, and has
    similar allocation figures:

我们把堆消耗换成栈消耗了！
事实上，如果我们用 ``-K`` 标志增加栈的大小到前面堆的大小，程序就能够运行完成，
并生成相似的内存分配图：

::

    $ ghc -O2 --make B.hs -prof -auto-all -caf-all -fforce-recomp
    [1 of 1] Compiling Main             ( B.hs, B.o )
    Linking B ...
    $ time ./B 1e6 +RTS -i0.001 -hc -p -K100M
    500000.5
    ./B 1e6 +RTS -i0.001 -hc -p -K100M  38.70s user 0.27s system 99% cpu 39.241 total


..
    Generating the heap profile, we see all the allocation is now in mean:

从生成的堆剖析结果中，我们可以看到现在 ``mean`` 的整个内存分配状况：

..
    [img here]
.. image:: ../image/ch25/ch25-stack.png

..
    The question is: why are we building up more and more allocated state,
    when all we are doing is folding over the list? This, it turns out,
    is a classic space leak due to excessive laziness.

问题是：为什么即使采用折叠的方式，程序仍然会引发越来越多的内存分配呢？
其实，这就是典型的极度惰性(excessive laziness)带来的空间泄露问题。

..
    Strictness and tail recursion
    +++++++++++++++++++++++++++++

严格执行和尾递归
+++++++++++

..
    The problem is that our left-fold, foldl, is too lazy. What we want
    is a tail recursive loop, which can be implemented effectively as
    a goto, with no state left on the stack. In this case though, rather
    than fully reducing the tuple state at each step, a long chain of
    thunks is being created, that only towards the end of the program
    is evaluated. At no point do we demand reduction of the loop state,
    so the compiler is unable to infer any strictness, and must reduce
    the value purely lazily.

产生问题的原因是，我们的左折叠函数 ``foldl`` 是惰性的。
我们想要的是一个尾递归循环，实现的像 goto 一样高效而没有保留在栈上的状态。
而现在的情况并不是在每一步都会消掉状态元组，而是产生一个 thunk 的长链，只会在程序结束时才会求值。
在任何时候我们都没有要求减少循环状态，所以编译器无法推断出什么时候必须严格执行，以减少纯惰性的值。

..
    What we need to do is to tune the evaluation strategy slightly:
    lazily unfolding the list, but strictly accumulating the fold state.
    The standard approach here is to replace foldl with foldl', from the
    Data.List module:

所以，我们要稍微地调整求值的策略：惰性地展开列表，但是严格地累加折叠状态。
标准方法是使用 ``Data.List`` 模块的 ``foldl'`` 替换 ``foldl``：

::

    -- file: ch25/C.hs
    mean :: [Double] -> Double
    mean xs = s / fromIntegral n
      where
        (n, s)     = foldl' k (0, 0) xs
        k (n, s) x = (n+1, s+x)


..
    However, if we run this implementation, we see we still haven't
    quite got it right:

然而这个程序的运行方式仍然和我们想象中的不一样：

::

    $ ghc -O2 --make C.hs
    [1 of 1] Compiling Main             ( C.hs, C.o )
    Linking C ...
    $ time ./C 1e6
    Stack space overflow: current size 8388608 bytes.
    Use `+RTS -Ksize' to increase it.
    ./C 1e6  0.44s user 0.13s system 94% cpu 0.601 total

..
    Still not strict enough! Our loop is continuing to accumulate
    unevaluated state on the stack. The problem here is that foldl'
    is only outermost strict:

计算还是不够严格！我们的循环还是继续在栈上累积没有求值的折叠状态。
这里的问题是 ``foldl'`` 只在外部严格执行：

::

    -- file: ch25/Foldl.hs
    foldl' :: (a -> b -> a) -> a -> [b] -> a
    foldl' f z xs = lgo z xs
        where lgo z []     = z
              lgo z (x:xs) = let z' = f z x in z' `seq` lgo z' xs


..
    This loop uses `seq` to reduce the accumulated state at each step, but only to the outermost constructor on the loop state. That is, seq reduces an expression to "weak head normal form". Evaluation stops on the loop state once the first constructor is reached. In this case, the outermost constructor is the tuple wrapper, (,), which isn't deep enough. The problem is still the unevaluated numeric state inside the tuple.

这个循环虽然使用 ``seq`` 消减每步的累加状态，但是只对循环状态上最外部的 pair 构造器进行了严格执行。
也就是说，``seq`` 把一个表达式消减到“weak head normal form”，仅仅对第一个匹配的构造器严格求值。
在这种情况下，对于最外部的构造器元组 ``(,)`` 来说深度是不够的。
也就是说，现在的问题是 pair 中的元素仍然在未求值状态。

..
    Adding strictness
    +++++++++++++++++

增加严格执行
++++++++

..
    There are a number of ways to make this function fully strict. We can, for example, add our own strictness hints to the internal state of the tuple, yielding a truly tail recursive loop:

有很多方式可以使函数完全地严格执行。
例如，我们可以自己在 pair 的内部补充上严格求值的代码，就可以得到一个真正的尾递归循环：

::

    -- file: ch25/D.hs
    mean :: [Double] -> Double
    mean xs = s / fromIntegral n
      where
        (n, s)     = foldl' k (0, 0) xs
        k (n, s) x = n `seq` s `seq` (n+1, s+x)

..
    In this variant, we step inside the tuple state, and explicitly tell the compiler that each state component should be reduced, on each step. This gives us a version that does, at last, run in constant space:

在这次修改中，我们深入到状态元组中，明确地告诉编译器状态的各个部分在每一步都应该消耗掉。
最终，我们得到一个常量内存空间的版本：

::

    $ ghc -O2 D.hs --make
    [1 of 1] Compiling Main             ( D.hs, D.o )
    Linking D ...

..
    If we run this, with allocation statistics enabled, we get the satisfying result:

在打开内存分配统计的情况下运行这个程序，我们终于得到了满意的结果：

::

    $ time ./D 1e6 +RTS -sstderr
    ./D 1e6 +RTS -sstderr
    500000.5
    256,060,848 bytes allocated in the heap
         43,928 bytes copied during GC (scavenged)
         23,456 bytes copied during GC (not scavenged)
         45,056 bytes maximum residency (1 sample(s))

            489 collections in generation 0 (  0.00s)
              1 collections in generation 1 (  0.00s)

              1 Mb total memory in use

      INIT  time    0.00s  (  0.00s elapsed)
      MUT   time    0.12s  (  0.13s elapsed)
      GC    time    0.00s  (  0.00s elapsed)
      EXIT  time    0.00s  (  0.00s elapsed)
      Total time    0.13s  (  0.13s elapsed)

      %GC time       2.6%  (2.6% elapsed)

      Alloc rate    2,076,309,329 bytes per MUT second

      Productivity  97.4% of total user, 94.8% of total elapsed

    ./D 1e6 +RTS -sstderr  0.13s user 0.00s system 95% cpu 0.133 total

..
    Unlike our first version, this program is 97.4% efficient, spending only 2.6% of its time doing garbage collection, and it runs in a constant 1 megabyte of space. It illustrates a nice balance between mixed strict and lazy evaluation, with the large list unfolded lazily, while we walk over it, strictly. The result is a program that runs in constant space, and does so quickly.

不像我们的第一个版本那样，这个程序的计算效率是 97.4%，在 GC 上仅花费 2.6% 的时间，并且运行的内存是常量 1 兆。
它很好地展示了如何在同时使用严格计算和惰性计算之间取得平衡：对大型列表惰性展开，而在展开中严格求值。
这样就能得到一个使用常量的空间，并且运行速度很快的程序。

..
    Normal form reduction
    +++++++++++++++++++++

Normal form reduction
+++++++++++++++

..
    There are a number of other ways we could have addressed the strictness issue here. For deep strictness, we can use the rnf function, part of the parallel strategies library (along with using), which unlike seq reduces to the fully evaluated "normal form" (hence its name). Such a "deep seq" fold we can write as:

我们也有许多其他方法可以解决这里的严格执行问题。
比如对于深度严格求值，我们也可以使用并行策略库(parallel strategies library)里的 ``rnf`` 函数（以及与之相搭配的 ``using``）。
不像 ``seq`` 那样，``rnf`` 能够对表达式彻底求值成“normal form”（正如它的名字，即“reduced normal form”）。
使用“深度 ``seq``”我们可以这样重写折叠代码：

::

    -- file: ch25/E.hs
    import System.Environment
    import Text.Printf
    import Control.Parallel.Strategies

    main = do
        [d] <- map read `fmap` getArgs
        printf "%f\n" (mean [1..d])

    foldl'rnf :: NFData a => (a -> b -> a) -> a -> [b] -> a
    foldl'rnf f z xs = lgo z xs
        where
            lgo z []     = z
            lgo z (x:xs) = lgo z' xs
                where
                    z' = f z x `using` rnf

    mean :: [Double] -> Double
    mean xs = s / fromIntegral n
      where
        (n, s)     = foldl'rnf k (0, 0) xs
        k (n, s) x = (n+1, s+x) :: (Int, Double)

..
    We change the implementation of foldl' to reduce the state to normal form, using the rnf strategy. This also raises an issue we avoided earlier: the type inferred for the loop accumulator state. Previously, we relied on type defaulting to infer a numeric, integral type for the length of the list in the accumulator, but switching to rnf introduces the NFData class constraint, and we can no longer rely on defaulting to set the length type.

我们修改了 ``foldl'`` 的实现，使用 ``rnf`` 策略把状态规约到 normal form。
这样也引入了一个我们先前可以避免的问题：循环状态的类型推导。
程序原来可以通过默认的类型推导，把循环状态中列表的长度推导成数值整型。
但是在切换到 ``rnf`` 后，由于引入了 ``NFData`` 类型类的约束，我们就无法再依赖默认的类型推导了。

..
    Bang patterns
    +++++++++++++

Bang patterns
+++++++++

..
    Perhaps the cheapest way, syntactically, to add required strictness to code that's excessively lazy is via "bang patterns" (whose name comes from pronunciation of the "!" character as "bang"), a language extension introduced with the following pragma:

为一个极其惰性的代码添加严格执行，从语法修改的成本上来讲，大概最廉价的方式就是“bang patterns”了（它的名字来自符号“!”，发音是“bang”）。
我们可以用下面的编译指示引入的这个语言扩展：

::

    -- file: ch25/F.hs
    {-# LANGUAGE BangPatterns #-}

..
    With bang patterns, we can hint at strictness on any binding form, making the function strict in that variable. Much as explicit type annotations can guide type inference, bang patterns can help guide strictness inference. Bang patterns are a language extension, and are enabled with the BangPatterns language pragma. We can now rewrite the loop state to be simply:

通过 bang patterns 我们可以在把严格执行指示到任何 binding form 上，从而使函数在那个变量上严格执行。
和显示的类型标注可以指导类型推断一样，bang patterns 可以帮助指导推断严格执行。
现在我们可以把循环状态重写成更为简单的形式：

::

    -- file: ch25/F.hs
    mean :: [Double] -> Double
    mean xs = s / fromIntegral n
      where
        (n, s)       = foldl' k (0, 0) xs
        k (!n, !s) x = (n+1, s+x)

..
    The intermediate values in the loop state are now made strict, and the loop runs in constant space:

因为循环状态中的临时变量都是严格执行的，所以这个循环可以在常量空间中执行：

::

    $ ghc -O2 F.hs --make
    $ time ./F 1e6 +RTS -sstderr
    ./F 1e6 +RTS -sstderr
    500000.5
    256,060,848 bytes allocated in the heap
         43,928 bytes copied during GC (scavenged)
         23,456 bytes copied during GC (not scavenged)
         45,056 bytes maximum residency (1 sample(s))

            489 collections in generation 0 (  0.00s)
              1 collections in generation 1 (  0.00s)

              1 Mb total memory in use

      INIT  time    0.00s  (  0.00s elapsed)
      MUT   time    0.14s  (  0.15s elapsed)
      GC    time    0.00s  (  0.00s elapsed)
      EXIT  time    0.00s  (  0.00s elapsed)
      Total time    0.14s  (  0.15s elapsed)

      %GC time       0.0%  (2.3% elapsed)

      Alloc rate    1,786,599,833 bytes per MUT second

      Productivity 100.0% of total user, 94.6% of total elapsed

    ./F 1e6 +RTS -sstderr  0.14s user 0.01s system 96% cpu 0.155 total

..
    In large projects, when we are investigating memory allocation hot spots, bang patterns are the cheapest way to speculatively modify the strictness properties of some code, as they're syntactically less invasive than other methods.

在大型项目里，当我们正在侦查内存分配的热点时，
bang patterns 是最简单的一种试探性地修改代码的严格执行属性的方式。
因为它对语法的侵略性与其他方法相比更小。

..
    Strict data types
    +++++++++++++++++

严格的数据类型
+++++++++

..
    Strict data types are another effective way to provide strictness information to the compiler. By default, Haskell data types are lazy, but it is easy enough to add strictness information to the fields of a data type that then propagate through the program. We can declare a new strict pair type, for example:

严格的数据类型(strict data type)是另一个有效的方式提供给编译器严格执行的信息。
默认 Haskell 的数据类型都是惰性的，但是很容易通过为数据类型的字段添加严格执行的信息，
使严格执行推广到整个程序中。
例如，我们可以声明一个新的严格的 pair 类型：

::

    -- file: ch25/G.hs
    data Pair a b = Pair !a !b

..
    This creates a pair type whose fields will always be kept in weak head normal form. We can now rewrite our loop as:

这样定义的 pair 类型总是会将字段保存成 weak head normal form。
我们现在重写循环：

::

    -- file: ch25/G.hs
    mean :: [Double] -> Double
    mean xs = s / fromIntegral n
      where
        Pair n s       = foldl' k (Pair 0 0) xs
        k (Pair n s) x = Pair (n+1) (s+x)

..
    This implementation again has the same efficient, constant space behavior. At this point, to squeeze the last drops of performance out of this code, though, we have to dive a bit deeper.

这个实现再次不仅同样高效，而且只占用常量空间。
到了这个地方，为了能够从这个代码里榨干最后几滴性能，我们还需要进一步深入下去。

..
    Understanding Core
    ------------------

理解核心语言
----------------------

..
    Besides looking at runtime profiling data, one sure way of determining exactly what your program is doing is to look at the final program source after the compiler is done optimizing it, particularly in the case of Haskell compilers, which can perform very aggressive transformations on the code. GHC uses what is humorously referred to as "a simple functional language", known as Core, as the compiler intermediate representation. It is essentially a subset of Haskell, augmented with unboxed data types (raw machine types, directly corresponding to primitive data types in languages like C), suitable for code generation. GHC optimizes Haskell by transformation, repeatedly rewriting the source into more and more efficient forms. The Core representation is the final functional version of your program, before translation to low level imperative code. In other words, Core has the final say, and if all-out performance is your goal, it is worth understanding.

除了查看运行时剖析的数据外，一个可靠地了解你的程序运行过程的方法是，查看编译器优化后的最终程序源码。
特别是在 Haskell 编译器会对源码进行各种非常激进的转换(transform)的情况下，这一方法将非常有效。
GHC 使用一种被幽默地称为“一门简单的函数式语言”的核心语言(Core，以下也会译成“核心代码”)来作为编译器的中间表示。
它实质上是适合代码生成的 Haskell 语言的子集，并扩增了一种未封装数据类型(unboxed data type)，也就是原生机器类型。
就像 C 语言一样直接对应着机器的基本数据类型(primitive data type)。
GHC 通过转换来优化 Haskell，反复重写源码以得到更有效率的形式。
在被翻译成底层命令式代码之前，核心代码就是你的代码在函数式上的最终版。
换句话说，核心代码具有最终发言权。
如果你的目标是让程序到达终极性能的话，那么你很值得去掌握它。

[译注： 对于未封装类型(unboxed types)的解释，从 wiki 上做以下简单摘要：
GHC 中的大多数类型是封装的(boxed)，也就是意味着这个类型的值实际上是用一个指向堆对象(heap object)的指针表示的。
比如 Haskell 中的 Int，其实是一个占用两个字大小的堆对象。
而对于一个未封装类型，它的值就是它本身，既没有指针也没有在堆上的内存分配。]

..
    To view the Core version of our Haskell program we compile with the -ddump-simpl flag, or use the ghc-core tool, a third-party utility that lets us view Core in a pager. So let's look at the representation of our final fold using strict data types, in Core form:

为了查看我们的 Haskell 程序的核心代码，需要编译时使用 ``-ddump-simpl`` 标志。
也可以使用一个叫做 ghc-core 的第三方工具，使我们能够分页地查看核心代码。

::

    $ ghc -O2 -ddump-simpl G.hs

..
    A screenful of text is generated. If we look carefully at, we'll see a loop (here, cleaned up slightly for clarity):

生成了满屏幕的文本。
仔细看一下，我们会发现一个循环（在此为了能够看得清晰，就稍微清理了一下）

::

    lgo :: Integer -> [Double] -> Double# -> (# Integer, Double #)

    lgo = \ n xs s ->
        case xs of
          []       -> (# n, D# s #);
          (:) x ys ->
            case plusInteger n 1 of
                n' -> case x of
                    D# y -> lgo n' ys (+## s y)

..
    This is the final version of our foldl', and tells us a lot about the next steps for optimization. The fold itself has been entirely inlined, yielding an explicit recursive loop over the list. The loop state, our strict pair, has disappeared entirely, and the function now takes its length and sum accumulators as direct arguments along with the list.

这就是 ``foldl'`` 的最终版本，它告诉了我们很多下一步的优化信息。
折叠已经完全内联，生成一个显式的列表递归循环。
循环状态，也就是我们的严格执行的 pair 已经完全消失。
现在函数已经把列表、长度以及总和一起作为参数使用。

..
    The sum of the list elements is represented with an unboxed Double# value, a raw machine double kept in a floating point register. This is ideal, as there will be no memory traffic involved keeping the sum on the heap. However, the length of the list, since we gave no explicit type annotation, has been inferred to be a heap-allocated Integer, with requires a non-primitive plusInteger to perform addition. If it is algorithmically sound to use a Int instead, we can replace Integer with it, via a type annotation, and GHC will then be able to use a raw machine Int# for the length. We can hope for an improvement in time and space by ensuring both loop components are unboxed, and kept in registers.

列表元素的总和被表示成未封装的 Double# 类型，在原生机器里，会被保存在一个浮点寄存器上。
这样做非常好！因为这样就可以避免使用堆上的变量而带来的一系列内存操作。
然而对于列表的长度，因为我们没有给它显式的类型标注，所以它被推断为分配在堆上的 Integer，并附带了一个非基本数据类型需要的 ``plusInteger`` 实施相加操作。
如果在算法的合理性上我们可以用 Int 替代 Integer，那么通过使用类型标注，GHC 就会用一个原生机器的 Int# 类型作为长度。
通过确保循环状态的分量都是保存在寄存器上的未封装类型， 程序的时间和空间性能将有机会获得更大的提升。

..
    The base case of the loop, its end, yields an unboxed pair (a pair allocated only in registers), storing the final length of the list, and the accumulated sum. Notice that the return type is a heap-allocated Double value, indicated by the D# constructor, which lifts a raw double value onto the heap. Again this has implications for performance, as GHC will need to check that there is sufficient heap space available before it can allocate and return from the loop.

循环的基础 case 分支，也就是循环的结束，生成一个未封装 pair（只分配在寄存器上的 pair），保存着列表的最终长度以及累加的总和。
注意返回类型是堆分配的 Double 类型，这点可以从 D# 构造器看的出来。
所以它在堆内存上占据一个原生的双精度值大小的空间。
再次，这里也会对性能产生影响，因为在为它分配内存和返回循环之前， GHC 需要检查是否有足够的堆空间。

..
    We can avoid this final heap check by having GHC return an unboxed Double# value, which can be achieved by using a custom pair type in the loop. In addition, GHC provides an optimiztion that unboxes the strict fields of a data type, ensuring the fields of the new pair type will be stored in registers. This optimization is turned on with -funbox-strict-fields.

我们可以避免这个最终的堆检查， 通过使 GHC 返回一个未封装的 Double# 类型。
这点可以用一个自定义的 pair 类型实现。
此外，GHC 提供了一个对数据类型中的严格字段不封装的优化，
这样就保证了新 pair 类型的字段会被保存在寄存器上。
这个优化通过 ``-funbox-strict-fields`` 标志开启。

..
    We can make both representation changes by replacing the polymorphic strict pair type with one whose fields are fixed as Int and Double:

对于上面提到的两个代码改进，我们可以通过使用固定有 Int 和 Double 类型的字段的 pair 数据类型，来替换原先的多态严格的 pair 来一并完成。

::

    -- file: ch25/H.hs
    data Pair = Pair !Int !Double

    mean :: [Double] -> Double
    mean xs = s / fromIntegral n
      where
        Pair n s       = foldl' k (Pair 0 0) xs
        k (Pair n s) x = Pair (n+1) (s+x)

..
    Compiling this with optimizations on, and -funbox-strict-fields -ddump-simpl, we get a tighter inner loop in Core:

带着优化标志和 ``-funbox-strict-fields -ddump-simpl`` 编译程序，
我们得到一个拥有更紧密的内循环的核心代码：

::

    lgo :: Int# -> Double# -> [Double] -> (# Int#, Double# #)
    lgo = \ n s xs ->
        case xs of
          []       -> (# n, s #)
          (:) x ys ->
            case x of
                D# y -> lgo (+# n 1) (+## s y) ys

..
    Now the pair we use to represent the loop state is represented and returned as unboxed primitive types, and will be kept in registers. The final version now only allocates heap memory for the list nodes, as the list is lazily demanded. If we compile and run this tuned version, we can compare the allocation and time performance against our original program:

现在，循环状态的 pair 已经作为未封装的基本类型来表示和返回了，也就是说它将会保存在寄存器上使用。
最终版本的程序只剩下列表的节点被分配在堆内存上，这样做是为了满足列表的惰性计算需要。
编译和运行这个调优过的版本，我们可以和原来的程序对比一下内存分配和时间的性能。

::

    $ time ./H 1e7 +RTS -sstderr
    ./H 1e7 +RTS -sstderr
    5000000.5
    1,689,133,824 bytes allocated in the heap
        284,432 bytes copied during GC (scavenged)
             32 bytes copied during GC (not scavenged)
         45,056 bytes maximum residency (1 sample(s))

           3222 collections in generation 0 (  0.01s)
              1 collections in generation 1 (  0.00s)

              1 Mb total memory in use

      INIT  time    0.00s  (  0.00s elapsed)
      MUT   time    0.63s  (  0.63s elapsed)
      GC    time    0.01s  (  0.02s elapsed)
      EXIT  time    0.00s  (  0.00s elapsed)
      Total time    0.64s  (  0.64s elapsed)

      %GC time       1.0%  (2.4% elapsed)

      Alloc rate    2,667,227,478 bytes per MUT second

      Productivity  98.4% of total user, 98.2% of total elapsed

    ./H 1e7 +RTS -sstderr  0.64s user 0.00s system 99% cpu 0.644 total


..
    While our original program, when operating on a list of 10 million elements, took more than a minute to run, and allocated more than 700 megabytes of memory, the final version, using a simple higher order fold, and a strict data type, runs in around half a second, and allocates a total of 1 megabyte. Quite an improvement!

当操作的列表包含 1 千万个元素时，原来的程序运行了超过 1 分钟的时间，并且分配了超过 700M 的内存。
然而使用一个简单的高阶折叠，以及一个严格的数据类型的最终版本的程序，却运行了大概半秒钟，并分配了总共 1M 内存。
这可真是相当大的进步啊！

..
    The general rules we can learn from the profiling and optimization process are:

我们可以从剖析和优化过程中学到的通用规则有：

..
    - Compile to native code, with optimizations on
    - When in doubt, use runtime statistics, and time profiling
    - If allocation problems are suspected, use heap profiling
    - A careful mixture of strict and lazy evaluation can yield the best results
    - Prefer strict fields for atomic data types (Int, Double and similar types)
    - Use data types with simpler machine representations (prefer Int over Integer)

- 编译成本地程序时，要打开优化标志
- 当有疑虑，就使用运行时统计和时间剖析
- 如果怀疑有内存分配的问题，就使用堆剖析
- 小心翼翼地结合严格和惰性执行会产生最好的结果
- 最好对原子数据类型（如 Int、Double 等相似类型）的字段标记成严格执行
- 要使用能够表示成原生机器的基本数据类型的数据类型（最好用 Int 而不是 Integer）

..
    These simple strategies are enough to identify and squash untoward memory use issues, and when used wisely, can avoid them occurring in the first place.

这些简单的规则足以定位和消除麻烦的内存占用问题，正确地使用这些规则还能够有效地防范这些问题。

..
    Advanced techniques: fusion
    ---------------------------

高级技术：fusion(融合)
----------------------------

..
    The final bottleneck in our program is the lazy list itself. While we can avoid allocating it all at once, there is still memory traffic each time around the loop, as we demand the next cons cell in the list, allocate it to the heap, operate on it, and continue. The list type is also polymorphic, so the elements of the list will be represented as heap allocated Double values.

这个程序最后的性能瓶颈是惰性列表本身。
虽然我们避免了立刻把它所需的内存全部分配出来，但也因此带来了每次循环时内存操作的消耗。
因为在循环中我们要取下一个列表的头元素，在堆上分配内存给它，操作，然后继续。
这个列表类型也是多态的，所以它的元素将表示成分配在堆上的 Double 值。

..
    What we'd like to do is eliminate the list entirely, keeping just the next element we need in a register. Perhaps surprisingly, GHC is able to transform the list program into a listless version, using an optimization known as deforestation, which refers to a general class of optimizations that involve eliminating intermediate data structures. Due to the absence of side effects, a Haskell compiler can be extremely aggressive when rearranging code, reordering and transforming wholesale at times. The specific deforestation optimization we will use here is stream fusion.

我们想做的是完全去掉列表，仅在寄存器上保存我们需要的下一个元素。
可能会使人感到意外，GHC 能够把一个包含列表的程序转换成无列表的形式，用一个被称为 deforestation 的优化。
这个优化是指一类去掉中间数据结构的优化。
得益于 Haskell 代码不包含副作用，Haskell 编译器有时会非常激进的大规模地重排、调整顺序和转换代码。
这次我们使用的 deforestation 优化叫做 stream fusion(流融合)。

..
    This optimization transforms recursive list generation and transformation functions into non-recursive unfolds. When an unfold appears next to a fold, the structure between them is then eliminated entirely, yielding a single, tight loop, with no heap allocation. The optimization isn't enabled by default, and it can radically change the complexity of a piece of code, but is enabled by a number of data structure libraries, which provide "rewrite rules", custom optimizations the compiler applies to functions the library exports.

这个优化可以把迭代列表生成(recursive list generation)和转换函数(transformation functions)转换成非迭代的展开(non-recursive unfolds)。
当一个展开靠近一个折叠时，它们之间的结构会被去掉，然后生成一个无需堆内存分配的单一紧凑的循环。
这个优化默认是没有开启的，因为它会急剧地增加一段代码的复杂度。
但是它在许多数据结构库上都开启了。
通过编译器提供的“重写规则”，自定义优化会应用到这些库的导出函数上。

..
    We'll use the uvector library, which provides a suite of list-like operations that use stream fusion to remove intermediate data structures. Rewriting our program to use streams is straightforward:

我们将使用 ``uvector`` 库，这个库提供了具有一整套类似列表操作的数据类型。
但与列表不同的是这些操作使用 stream fusion 移除了中间的数据结构。
把我们前面的程序改写成使用 streams 优化的代码并不困难：

::

    -- file: ch25/I.hs
    import System.Environment
    import Text.Printf
    import Data.Array.Vector

    main = do
        [d] <- map read `fmap` getArgs
        printf "%f\n" (mean (enumFromToFracU 1 d))

    data Pair = Pair !Int !Double

    mean :: UArr Double -> Double
    mean xs = s / fromIntegral n
      where
        Pair n s       = foldlU k (Pair 0 0) xs
        k (Pair n s) x = Pair (n+1) (s+x)

..
    After installing the uvector library, from Hackage, we can
    build our program, with -O2 -funbox-strict-fields, and inspect
    the Core that results:

从 Hackage 上安装过 ``uvector`` 库后，编译程序，使用 ``-O2 -funbox-strict-fields``，再检查核心代码：

::

    fold :: Int# -> Double# -> Double# -> (# Int#, Double# #)
    fold = \ n s t ->
        case >## t limit of {
          False -> fold (+# n 1) (+## s t) (+## t 1.0)
          True  -> (# n, s #)


..
    This is really the optimal result! Our lists have been entirely
    fused away, yielding a tight loop where list generation is
    interleaved with accumulation, and all input and output variables
    are kept in registers. Running this, we see another improvement
    bump in performance, with runtime falling by another order of magnitude:

这次真的是最优化的结果了！
我们的列表结构现在已经完全融化成一个紧凑的循环了。
它的列表生成是和循环状态累加相互交错(interleaved)，所有的输入和输出变量都保存在寄存器里。
运行这个程序，我们会看到程序性能再一次猛进，运行时消耗也下降到另一个数量级：

::

    $ time ./I 1e7
    5000000.5
    ./I 1e7  0.06s user 0.00s system 72% cpu 0.083 total

..
    Tuning the generated assembly
    +++++++++++++++++++++++++++++

调整生成的汇编代码
++++++++++++++++

..
    Given that our Core is now optimal, the only step left to
    take this program further is to look directly at the assembly.
    Of course, there are only small gains left to make at this point.
    To view the generated assembly, we can use a tool like ghc-core,
    or generate assembly to standard output with the -ddump-asm flag to GHC.
    We have few levers available to adjust the generated assembly,
    but we may choose between the C and native code backends to GHC,
    and, if we choose the C backend, which optimization flags to pass to GCC.
    Particularly with floating point code, it is sometimes useful to
    compile via C, and enable specific high performance C compiler optimizations.

考虑到现在我们的核心代码已是最优化了，继续深入下去就只剩下汇编代码了。
当然，到了这个地步可以获得的优化就剩下非常少了。
为了查看生成的汇编，我们可以使用比如像 ``ghc-core`` 这样的工具，
或者对 GHC 使用 ``-ddump-asm`` 标志把汇编代码生成到标准输出上。
我们几乎没有“控制杆”调整生成的汇编，但是我们可以在后端是 C 语言还是原生代码之间做选择。
以及，如果我们选 C 语言后端，我们还可以选择把哪些优化标志传给 GCC。
特别是在有浮点数代码的情况下，通过 C 语言编译，并开启特定的高性能的 C 编译器优化，会很有用。

..
    For example, we can squeeze out the last drops of performance from our
    final fused loop code by using -funbox-strict-fields -fvia-C -optc-O2,
    which cuts the running time in half again (as the C compiler is able to
    optimize away some redundant move instructions in the program's inner loop):

例如，通过 ``-funbox-strict-fields -fvia-C -optc-O2`` 这些标志，我们还可以从最终融合过的循环代码中榨出最后几滴性能。
它可以再次减少一半运行时间（因为 C 编译器能够优化掉一些内循环中冗余的移动指令）：

::

    $ ghc -fforce-recomp --make -O2 -funbox-strict-fields -fvia-C -optc-O2 I.hs
    [1 of 1] Compiling Main             ( I.hs, I.o )
    Linking I ...
    $ time ./I 1e7
    5000000.5
    ./I 1e7  0.04s user 0.00s system 98% cpu 0.047 total

..
    Inspecting the final x86_64 assembly (via -keep-tmp-files),
    we see the generated loop contains only six instructions:

检查最终 x86_64 汇编（通过 ``-keep-tmp-files``），我们看到生成的循环只包含 6 个指令：

::

    go:
      ucomisd     5(%rbx), %xmm6
      ja  .L31
      addsd       %xmm6, %xmm5
      addq        $1, %rsi
      addsd       .LC0(%rip), %xmm6
      jmp go

..
    We've effectively massaged the program through multiple
    source-level optimizations, all the way to the final assembly.
    There's nowhere else to go from here. Optimising code to this level
    is very rarely necessary, of course, and typically only makes sense
    when writing low level libraries, or optimizing particularly important code,
    where all algorithm choices have already been determined. For
    day-to-day code, choosing better algorithms is always a more effective
    strategy, but it's useful to know we can optimize down to the metal if necessary.

通过多种源码层的优化我们已经有力地蹂躏了程序，所有这些也促成了最终的汇编代码。
至此就没有其他路可以走了。
很少有必要优化代码到这个级别。
当然，典型情况下只有在写底层的库时才有意义，又或者在已经决定好最终算法的情况下优化特别重要的代码。
对日常的代码，选择更好的算法总是更有效的策略。
但是清楚如果有必要我们可以优化到底线的程度也是很有用的。

..
    Conclusions
    -----------

结论
-----

..
    In this chapter we've looked at a suite of tools and techniques
    you can use to track down and identify problematic areas of your code,
    along with a variety of conventions that can go a long way towards
    keeping your code lean and efficient. The goal is really to program
    in such a way that you have good knowledge of what your code is doing,
    at all levels from source, through the compiler, to the metal,
    and be able to focus in on particular levels when requirements demand.

这一章向我们展示了如何使用一套工具和技术去跟踪和定位代码的问题区域，以及列出了一些可以保持代码简洁高效的惯例。
我们的目标是能够从源码、编译器直到底层为止的各个层面上了解代码的具体行为，并在有需要时将注意力集中到特定的层面上。

..
    By sticking to simple rules, choosing the right data structures,
    and avoiding the traps of the unwary, it is perfectly possible to
    reliably achieve high performance from your Haskell code, while being
    able to develop at a very high level. The result is a sweet balance
    of productivity and ruthless efficiency.

通过坚持简单的规则、选择正确的数据结构并避免不谨慎的陷阱，
即便是在非常高的层次之上进行开发，使用 Haskell 获得高性能也是完全有可能的。
最终的结果将会是一个在生产率和无情的性能之间美妙的平衡。
